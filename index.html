<!DOCTYPE html>
<html lang="ja">
<head>
<meta charset="UTF-8">
<title>YOLOv7 ONNX + Webカメラ</title>
<style>
  body { text-align: center; background: #222; color: white; }
  video, canvas { border: 2px solid #555; border-radius: 8px; }
</style>
</head>
<body>
<h1>YOLOv7 Webカメラ検出</h1>
<video id="video" width="640" height="480" autoplay playsinline></video>
<canvas id="canvas" width="640" height="480"></canvas>
<p id="status">モデル読み込み中...</p>

<script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
<script>
const video = document.getElementById('video');
const canvas = document.getElementById('canvas');
const ctx = canvas.getContext('2d');
const statusEl = document.getElementById('status');

let session = null;
let labels = [];

// Webカメラ起動
async function initCamera() {
  const stream = await navigator.mediaDevices.getUserMedia({ video: true });
  video.srcObject = stream;
  await new Promise(r => video.onloadedmetadata = r);
}

// モデルとラベル読み込み
async function loadModel() {
  session = await ort.InferenceSession.create('./yolov7.onnx');
  const res = await fetch('./labels.txt');
  labels = (await res.text()).split('\n').map(l => l.trim()).filter(l => l);
  statusEl.innerText = "モデル読み込み完了";
}

// 推論処理
async function detectFrame() {
  if (!session) return;
  const inputSize = 640;
  
  // video → canvasにコピー
  ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
  
  // 画像データ取得 & リサイズ
  let imgData = ctx.getImageData(0, 0, canvas.width, canvas.height);
  let tempCanvas = document.createElement('canvas');
  tempCanvas.width = inputSize;
  tempCanvas.height = inputSize;
  let tctx = tempCanvas.getContext('2d');
  tctx.drawImage(video, 0, 0, inputSize, inputSize);
  let resized = tctx.getImageData(0, 0, inputSize, inputSize);
  
  // CHW float32正規化
  let input = new Float32Array(inputSize * inputSize * 3);
  for (let i = 0; i < inputSize * inputSize; i++) {
    input[i] = resized.data[i * 4] / 255.0;       // R
    input[i + inputSize * inputSize] = resized.data[i * 4 + 1] / 255.0; // G
    input[i + inputSize * inputSize * 2] = resized.data[i * 4 + 2] / 255.0; // B
  }
  
  const tensor = new ort.Tensor('float32', input, [1, 3, inputSize, inputSize]);
  
  // 推論
  const results = await session.run({ images: tensor });
  const output = results[Object.keys(results)[0]].data;
  
  drawDetections(output, inputSize);
  
  requestAnimationFrame(detectFrame);
}

// 検出結果描画
function drawDetections(output, inputSize) {
  ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
  const numClasses = labels.length;
  const boxes = [];
  
  for (let i = 0; i < output.length; i += (5 + numClasses)) {
    const [x, y, w, h, objScore, ...classScores] = output.slice(i, i + 5 + numClasses);
    const score = objScore * Math.max(...classScores);
    if (score > 0.4) {
      const classId = classScores.indexOf(Math.max(...classScores));
      const label = labels[classId];
      
      const cx = x * canvas.width / inputSize;
      const cy = y * canvas.height / inputSize;
      const bw = w * canvas.width / inputSize;
      const bh = h * canvas.height / inputSize;
      
      ctx.strokeStyle = "lime";
      ctx.lineWidth = 2;
      ctx.strokeRect(cx - bw / 2, cy - bh / 2, bw, bh);
      
      ctx.fillStyle = "lime";
      ctx.font = "16px Arial";
      ctx.fillText(`${label} ${(score*100).toFixed(1)}%`, cx - bw / 2, cy - bh / 2 - 5);
    }
  }
}

(async () => {
  await initCamera();
  await loadModel();
  detectFrame();
})();
</script>
</body>
</html>
